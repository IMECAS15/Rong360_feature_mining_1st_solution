{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/sample_train.txt\", delimiter=\"\\t\")\n",
    "test = pd.read_csv(\"../data/test_id.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = pd.read_csv(\"../features/risk/risk.csv\")\n",
    "df = df.merge(risk, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = pd.read_csv(\"../features/symbol/symbol.csv\")\n",
    "df = df.merge(symbol, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_emb = pd.read_csv(\"../features/graph/graph_filtered_64.emb\")\n",
    "# df = df.merge(dp_emb, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_emb = pd.read_csv(\"../features/graph/deepwalk_192_filtered.emb\")\n",
    "df = df.merge(dp_emb, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28442, 193)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nv_emb = pd.read_csv(\"../data/edge/node2vec_64.emb\")\n",
    "# df = df.merge(nv_emb, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_info = pd.read_csv(\"../features/graph/graph_info.csv\")\n",
    "df = df.merge(graph_info, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_pca = pd.read_csv(\"../data/app/app_pca_16.csv\")\n",
    "# df = df.merge(app_pca, on=\"id\", how=\"left\")\n",
    "\n",
    "app_lda = pd.read_csv(\"../features/app/app_lda_16.csv\")\n",
    "df = df.merge(app_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# app_lda = pd.read_csv(\"../data1/app/app_lda_16.csv\")\n",
    "# df = df.merge(app_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# app_nmf = pd.read_csv(\"../data/app/app_nmf_16.csv\")\n",
    "# df = df.merge(app_nmf, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_info = pd.read_csv(\"../features/app/app_info.csv\")\n",
    "# df = df.merge(app_info, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pca = pd.read_csv(\"../features/app_graph/to_app_pca_mean.csv\")\n",
    "# from_pca = pd.read_csv(\"../features/app_graph/from_app_pca_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca, on=\"id\", how=\"left\")\n",
    "\n",
    "to_pca = pd.read_csv(\"../features/app_graph/to_app_pca_num_mean.csv\")\n",
    "from_pca = pd.read_csv(\"../features/app_graph/from_app_pca_num_mean.csv\")\n",
    "\n",
    "df = df.merge(to_pca, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_pca, on=\"id\", how=\"left\")\n",
    "\n",
    "to_pca = pd.read_csv(\"../features/app_graph/to_app_pca_weight_mean.csv\")\n",
    "from_pca = pd.read_csv(\"../features/app_graph/from_app_pca_weight_mean.csv\")\n",
    "\n",
    "df = df.merge(to_pca, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_pca, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pca_nmf = pd.read_csv(\"../features/app_graph/to_app_nmf_mean.csv\")\n",
    "# from_pca_nmf = pd.read_csv(\"../features/app_graph/from_app_nmf_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca_nmf, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca_nmf, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_pca_nmf = pd.read_csv(\"../features/app_graph/to_app_nmf_num_mean.csv\")\n",
    "# from_pca_nmf = pd.read_csv(\"../features/app_graph/from_app_nmf_num_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca_nmf, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca_nmf, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_pca_nmf = pd.read_csv(\"../features/app_graph/to_app_nmf_weight_mean.csv\")\n",
    "# from_pca_nmf = pd.read_csv(\"../features/app_graph/from_app_nmf_weight_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca_nmf, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca_nmf, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_lda = pd.read_csv(\"../features/app_graph/to_app_lda_mean.csv\")\n",
    "# from_lda = pd.read_csv(\"../features/app_graph/from_app_lda_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_lda, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_lda = pd.read_csv(\"../features/app_graph/to_app_lda_num_mean.csv\")\n",
    "# from_lda = pd.read_csv(\"../features/app_graph/from_app_lda_num_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_lda, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_lda = pd.read_csv(\"../features/app_graph/to_app_lda_weight_mean.csv\")\n",
    "# from_lda = pd.read_csv(\"../features/app_graph/from_app_lda_weight_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_lda, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_lda, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "risk_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_risk = pd.read_csv(\"../features/risk_graph/to_risk_mean.csv\")\n",
    "# from_risk = pd.read_csv(\"../features/risk_graph/from_risk_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_num_mean.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_num_mean.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_weight_mean.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_weight_mean.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_risk = pd.read_csv(\"../features/risk_graph/to_risk_sum.csv\")\n",
    "# from_risk = pd.read_csv(\"../features/risk_graph/from_risk_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_num_sum.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_num_sum.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_weight_sum.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_weight_sum.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "symbol_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_sum.csv\")\n",
    "# from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_num_sum.csv\")\n",
    "# from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_num_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_cat_weight = pd.read_csv(\"../features/symbol_graph/to_symbol_weight_sum.csv\")\n",
    "# from_cat_weight = pd.read_csv(\"../features/symbol_graph/from_symbol_weight_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_weight, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_weight, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_mean.csv\")\n",
    "# from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_num_mean.csv\")\n",
    "from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_num_mean.csv\")\n",
    "\n",
    "df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "to_cat_weight = pd.read_csv(\"../features/symbol_graph/to_symbol_weight_mean.csv\")\n",
    "from_cat_weight = pd.read_csv(\"../features/symbol_graph/from_symbol_weight_mean.csv\")\n",
    "\n",
    "df = df.merge(to_cat_weight, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_cat_weight, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_3.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_10.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_30.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_100.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_300.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中心性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = pd.read_csv(\"../features/graph/pagerank.csv\")\n",
    "df = df.merge(pr, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [\"to\", \"from\"]:\n",
    "    for w in ['', '_num', '_weight']:\n",
    "        for f in ['sum', 'mean']:\n",
    "            a = pd.read_csv(\"../features/graph/%s_pagerank%s_%s.csv\" % (d, w, f))\n",
    "            df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = pd.read_csv(\"../features/graph/hits.csv\")\n",
    "df = df.merge(hits, on=\"id\", how=\"left\")\n",
    "\n",
    "for d in [\"to\", \"from\"]:\n",
    "    for w in ['', '_num', '_weight']:\n",
    "        for f in ['sum', 'mean']:\n",
    "            a = pd.read_csv(\"../features/graph/%s_hits%s_%s.csv\" % (d, w, f))\n",
    "            df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二度联系人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 联系人个数\n",
    "for d1 in [\"from\", \"to\"]:\n",
    "    for d2 in [\"from\", \"to\"]:\n",
    "        for t in [\"\", \"_num\", \"_weight\"]:\n",
    "            for f in [\"sum\", \"mean\"]:\n",
    "                a = pd.read_csv(\"../features/graph/%s_d2_%s%s_%s.csv\" % (d1, d2, t, f))\n",
    "                df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二度联系人PageRank值\n",
    "for d1 in [\"from\", \"to\"]:\n",
    "    for d2 in [\"from\", \"to\"]:\n",
    "        for t in [\"num\", \"weight\"]:\n",
    "            for f in [\"sum\", \"mean\"]:\n",
    "                a = pd.read_csv(\"../features/graph/%s_pg_%s_%s_%s_%s_%s.csv\" % (d1, d2, t, f, t, f))\n",
    "                df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d1 in [\"from\", \"to\"]:\n",
    "    for d2 in [\"from\", \"to\"]:\n",
    "        for t in [\"num\", \"weight\"]:\n",
    "            for f in [\"mean\"]:\n",
    "                a = pd.read_csv(\"../features/graph/%s_hits_%s_%s_%s_%s_%s.csv\" % (d1, d2, t, f, t, f))\n",
    "                df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  process_feature(train_x, valid_x, test_df):\n",
    "    result = []\n",
    "    drop_cols = ['id','label']\n",
    "    for df in [train_x, valid_x, test_df]:\n",
    "        result.append(df.drop(drop_cols, axis=1))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def cv(df, num_folds, param, model_dir, stratified=True, debug=False):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    train_df = df[df.label.notnull()]\n",
    "    test_df = df[df.label.isnull()]\n",
    "#     seed = param[\"seed\"]\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=178)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=178)\n",
    "#     del param['seed']\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    all_test_preds = []    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_df['label'])):\n",
    "        train_x, train_y = train_df.iloc[train_idx], train_df['label'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df.iloc[valid_idx], train_df['label'].iloc[valid_idx]\n",
    "        fold_preds = test_df[[\"id\"]]\n",
    "        \n",
    "        train_x, valid_x, test = process_feature(train_x, valid_x, test_df)\n",
    "        if n_fold == 0:\n",
    "            print(train_x.shape, valid_x.shape, test.shape)\n",
    "        \n",
    "        # xgb\n",
    "        train_data = xgb.DMatrix(train_x, label=train_y)\n",
    "        valid_data = xgb.DMatrix(valid_x, label=valid_y)\n",
    "        test_data = xgb.DMatrix(test)\n",
    "            \n",
    "        clf=xgb.train(params,\n",
    "                      train_data,\n",
    "                      num_boost_round=10000,\n",
    "                      evals=[(train_data, 'train'), (valid_data, 'eval')],\n",
    "                      maximize=True,\n",
    "                      early_stopping_rounds=50,\n",
    "                      verbose_eval=100)\n",
    "        valid_preds = clf.predict(valid_data)\n",
    "        test_preds = clf.predict(test_data)\n",
    "#         train_data = lgb.Dataset(train_x, label=train_y)\n",
    "#         validation_data = lgb.Dataset(valid_x, label=valid_y)\n",
    "\n",
    "#         clf=lgb.train(params,\n",
    "#                       train_data,\n",
    "#                       num_boost_round=10000,\n",
    "#                       valid_sets=[train_data, validation_data],\n",
    "#                       valid_names=[\"train\", \"valid\"],\n",
    "#                       early_stopping_rounds=100,\n",
    "#                       verbose_eval=100)\n",
    "\n",
    "#         valid_preds = clf.predict(valid_x, num_iteration=clf.best_iteration)\n",
    "#         test_preds = clf.predict(test, num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_preds['prob'] = test_preds\n",
    "        fold_preds['fold_id'] = n_fold + 1\n",
    "        all_test_preds.append(fold_preds)\n",
    "        \n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        \n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"feature\"] = train_x.columns.tolist()\n",
    "#         fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#         fold_importance_df[\"fold\"] = n_fold + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, valid_preds)))\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    full_auc = roc_auc_score(train_df['label'], oof_preds)\n",
    "    print('Full AUC score %.6f' % full_auc)      \n",
    "    if not debug:\n",
    "        train_df[\"prob\"] = oof_preds\n",
    "        train_df[['id', 'prob']].to_csv(model_dir + \"pred_train.csv\", index= False)\n",
    "\n",
    "        all_test_preds = pd.concat(all_test_preds, axis=0)\n",
    "        all_test_preds.to_csv(model_dir + \"all_test_preds.csv\", index=False)\n",
    "        \n",
    "        sub = pd.DataFrame()\n",
    "        sub['id'] = all_test_preds.id.unique()\n",
    "        sub.set_index(\"id\", inplace=True)\n",
    "        sub[\"prob\"] = all_test_preds.groupby(\"id\").prob.mean()\n",
    "        sub.reset_index().to_csv(model_dir + \"sub.txt\", index=False)\n",
    "#     display_importances(feature_importance_df)\n",
    "    return full_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"../model_output/xgb_652\"):\n",
    "    for root, dirs, files in os.walk(\"../model_output/xgb_652\", topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [\n",
    " {'colsample_bylevel': 0.8274559732797849, 'colsample_bytree': 0.6840193293100916, 'eval_metric': 'auc', 'learning_rate': 0.024217608001348066, 'max_depth': 8, 'reg_alpha': 12.389604101689343, 'reg_lambda': 644.3919081691505, 'seed': 5861}\",\n",
    " {'colsample_bylevel': 0.6727831071787986, 'colsample_bytree': 0.6646265542413805, 'eval_metric': 'auc', 'learning_rate': 0.020261116953314864, 'max_depth': 7, 'reg_alpha': 18.746421446281634, 'reg_lambda': 764.252592979315, 'seed': 835},\n",
    " {'colsample_bylevel': 0.8047667836255779, 'colsample_bytree': 0.615519357739198, 'eval_metric': 'auc', 'learning_rate': 0.022104345445937326, 'max_depth': 7, 'reg_alpha': 20.412650175445165, 'reg_lambda': 756.6634545842621, 'seed': 6433},\n",
    " {'colsample_bylevel': 0.6509039630401686, 'colsample_bytree': 0.6742793046599777, 'eval_metric': 'auc', 'learning_rate': 0.018420671588737513, 'max_depth': 9, 'reg_alpha': 25.886381796238048, 'reg_lambda': 940.456815943712, 'seed': 592},\n",
    " {'colsample_bylevel': 0.7210897816528165, 'colsample_bytree': 0.6241586621150841, 'eval_metric': 'auc', 'learning_rate': 0.021121029642481873, 'max_depth': 9, 'reg_alpha': 25.501646873406585, 'reg_lambda': 772.1386106247073, 'seed': 1418}\n",
    "]\n",
    "result = []\n",
    "for i, params in enumerate(params_list):\n",
    "    model_dir = \"../model_output/xgb_652/%d/\" % i\n",
    "    result.append(cv(df, 5, params, model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机数种子改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# params_list = [\n",
    "#  {'colsample_bylevel': 0.8274559732797849, 'colsample_bytree': 0.6840193293100916, 'eval_metric': 'auc', 'learning_rate': 0.024217608001348066, 'max_depth': 8, 'reg_alpha': 12.389604101689343, 'reg_lambda': 644.3919081691505, 'seed': 5861}\",\n",
    "#  {'colsample_bylevel': 0.6727831071787986, 'colsample_bytree': 0.6646265542413805, 'eval_metric': 'auc', 'learning_rate': 0.020261116953314864, 'max_depth': 7, 'reg_alpha': 18.746421446281634, 'reg_lambda': 764.252592979315, 'seed': 835},\n",
    "#  {'colsample_bylevel': 0.8047667836255779, 'colsample_bytree': 0.615519357739198, 'eval_metric': 'auc', 'learning_rate': 0.022104345445937326, 'max_depth': 7, 'reg_alpha': 20.412650175445165, 'reg_lambda': 756.6634545842621, 'seed': 6433},\n",
    "#  {'colsample_bylevel': 0.6509039630401686, 'colsample_bytree': 0.6742793046599777, 'eval_metric': 'auc', 'learning_rate': 0.018420671588737513, 'max_depth': 9, 'reg_alpha': 25.886381796238048, 'reg_lambda': 940.456815943712, 'seed': 592},\n",
    "#  {'colsample_bylevel': 0.7210897816528165, 'colsample_bytree': 0.6241586621150841, 'eval_metric': 'auc', 'learning_rate': 0.021121029642481873, 'max_depth': 9, 'reg_alpha': 25.501646873406585, 'reg_lambda': 772.1386106247073, 'seed': 1418}\n",
    "# ]\n",
    "# result = []\n",
    "# for i, params in enumerate(params_list):\n",
    "#     model_dir = \"../model_output/xgb_652/%d/\" % i\n",
    "#     result.append(cv(df, 5, params, model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70083962688755375, 0.69879064177483474, 0.69535315967479017, 0.69743125953054597, 0.6950223113031202]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
