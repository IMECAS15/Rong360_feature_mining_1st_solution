{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/sample_train.txt\", delimiter=\"\\t\")\n",
    "test = pd.read_csv(\"../data/test_id.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = pd.read_csv(\"../features/risk/risk.csv\")\n",
    "df = df.merge(risk, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = pd.read_csv(\"../features/symbol/symbol.csv\")\n",
    "df = df.merge(symbol, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_emb = pd.read_csv(\"../features/graph/graph_filtered_64.emb\")\n",
    "# df = df.merge(dp_emb, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_emb = pd.read_csv(\"../features/graph/deepwalk_128_filtered.emb\")\n",
    "df = df.merge(dp_emb, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nv_emb = pd.read_csv(\"../data/edge/node2vec_64.emb\")\n",
    "# df = df.merge(nv_emb, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_info = pd.read_csv(\"../features/graph/graph_info.csv\")\n",
    "df = df.merge(graph_info, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_pca = pd.read_csv(\"../data/app/app_pca_16.csv\")\n",
    "# df = df.merge(app_pca, on=\"id\", how=\"left\")\n",
    "\n",
    "app_lda = pd.read_csv(\"../features/app/app_lda_16.csv\")\n",
    "df = df.merge(app_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# app_lda = pd.read_csv(\"../data1/app/app_lda_16.csv\")\n",
    "# df = df.merge(app_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# app_nmf = pd.read_csv(\"../data/app/app_nmf_16.csv\")\n",
    "# df = df.merge(app_nmf, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_info = pd.read_csv(\"../features/app/app_info.csv\")\n",
    "# df = df.merge(app_info, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pca = pd.read_csv(\"../features/app_graph/to_app_pca_mean.csv\")\n",
    "# from_pca = pd.read_csv(\"../features/app_graph/from_app_pca_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca, on=\"id\", how=\"left\")\n",
    "\n",
    "to_pca = pd.read_csv(\"../features/app_graph/to_app_pca_num_mean.csv\")\n",
    "from_pca = pd.read_csv(\"../features/app_graph/from_app_pca_num_mean.csv\")\n",
    "\n",
    "df = df.merge(to_pca, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_pca, on=\"id\", how=\"left\")\n",
    "\n",
    "to_pca = pd.read_csv(\"../features/app_graph/to_app_pca_weight_mean.csv\")\n",
    "from_pca = pd.read_csv(\"../features/app_graph/from_app_pca_weight_mean.csv\")\n",
    "\n",
    "df = df.merge(to_pca, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_pca, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pca_nmf = pd.read_csv(\"../features/app_graph/to_app_nmf_mean.csv\")\n",
    "# from_pca_nmf = pd.read_csv(\"../features/app_graph/from_app_nmf_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca_nmf, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca_nmf, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_pca_nmf = pd.read_csv(\"../features/app_graph/to_app_nmf_num_mean.csv\")\n",
    "# from_pca_nmf = pd.read_csv(\"../features/app_graph/from_app_nmf_num_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca_nmf, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca_nmf, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_pca_nmf = pd.read_csv(\"../features/app_graph/to_app_nmf_weight_mean.csv\")\n",
    "# from_pca_nmf = pd.read_csv(\"../features/app_graph/from_app_nmf_weight_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_pca_nmf, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_pca_nmf, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_lda = pd.read_csv(\"../features/app_graph/to_app_lda_mean.csv\")\n",
    "# from_lda = pd.read_csv(\"../features/app_graph/from_app_lda_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_lda, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_lda = pd.read_csv(\"../features/app_graph/to_app_lda_num_mean.csv\")\n",
    "# from_lda = pd.read_csv(\"../features/app_graph/from_app_lda_num_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_lda, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_lda, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_lda = pd.read_csv(\"../features/app_graph/to_app_lda_weight_mean.csv\")\n",
    "# from_lda = pd.read_csv(\"../features/app_graph/from_app_lda_weight_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_lda, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_lda, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "risk_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_risk = pd.read_csv(\"../features/risk_graph/to_risk_mean.csv\")\n",
    "# from_risk = pd.read_csv(\"../features/risk_graph/from_risk_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_num_mean.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_num_mean.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_weight_mean.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_weight_mean.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_risk = pd.read_csv(\"../features/risk_graph/to_risk_sum.csv\")\n",
    "# from_risk = pd.read_csv(\"../features/risk_graph/from_risk_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_num_sum.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_num_sum.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")\n",
    "\n",
    "to_risk = pd.read_csv(\"../features/risk_graph/to_risk_weight_sum.csv\")\n",
    "from_risk = pd.read_csv(\"../features/risk_graph/from_risk_weight_sum.csv\")\n",
    "\n",
    "df = df.merge(to_risk, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_risk, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "symbol_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_sum.csv\")\n",
    "# from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_num_sum.csv\")\n",
    "# from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_num_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_cat_weight = pd.read_csv(\"../features/symbol_graph/to_symbol_weight_sum.csv\")\n",
    "# from_cat_weight = pd.read_csv(\"../features/symbol_graph/from_symbol_weight_sum.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_weight, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_weight, on=\"id\", how=\"left\")\n",
    "\n",
    "# to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_mean.csv\")\n",
    "# from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_mean.csv\")\n",
    "\n",
    "# df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "# df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "to_cat_num = pd.read_csv(\"../features/symbol_graph/to_symbol_num_mean.csv\")\n",
    "from_cat_num = pd.read_csv(\"../features/symbol_graph/from_symbol_num_mean.csv\")\n",
    "\n",
    "df = df.merge(to_cat_num, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_cat_num, on=\"id\", how=\"left\")\n",
    "\n",
    "to_cat_weight = pd.read_csv(\"../features/symbol_graph/to_symbol_weight_mean.csv\")\n",
    "from_cat_weight = pd.read_csv(\"../features/symbol_graph/from_symbol_weight_mean.csv\")\n",
    "\n",
    "df = df.merge(to_cat_weight, on=\"id\", how=\"left\")\n",
    "df = df.merge(from_cat_weight, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_3.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_10.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_30.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_100.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")\n",
    "\n",
    "# dp_knn = pd.read_csv(\"../features/knn/dp_knn_300.csv\")\n",
    "# df = df.merge(dp_knn, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中心性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = pd.read_csv(\"../features/graph/pagerank.csv\")\n",
    "df = df.merge(pr, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [\"to\", \"from\"]:\n",
    "    for w in ['', '_num', '_weight']:\n",
    "        for f in ['sum', 'mean']:\n",
    "            a = pd.read_csv(\"../features/graph/%s_pagerank%s_%s.csv\" % (d, w, f))\n",
    "            df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = pd.read_csv(\"../features/graph/hits.csv\")\n",
    "df = df.merge(hits, on=\"id\", how=\"left\")\n",
    "\n",
    "for d in [\"to\", \"from\"]:\n",
    "    for w in ['', '_num', '_weight']:\n",
    "        for f in ['sum', 'mean']:\n",
    "            a = pd.read_csv(\"../features/graph/%s_hits%s_%s.csv\" % (d, w, f))\n",
    "            df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二度联系人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 联系人个数\n",
    "for d1 in [\"from\", \"to\"]:\n",
    "    for d2 in [\"from\", \"to\"]:\n",
    "        for t in [\"\", \"_num\", \"_weight\"]:\n",
    "            for f in [\"sum\", \"mean\"]:\n",
    "                a = pd.read_csv(\"../features/graph/%s_d2_%s%s_%s.csv\" % (d1, d2, t, f))\n",
    "                df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二度联系人PageRank值\n",
    "for d1 in [\"from\", \"to\"]:\n",
    "    for d2 in [\"from\", \"to\"]:\n",
    "        for t in [\"num\", \"weight\"]:\n",
    "            for f in [\"sum\", \"mean\"]:\n",
    "                a = pd.read_csv(\"../features/graph/%s_pg_%s_%s_%s_%s_%s.csv\" % (d1, d2, t, f, t, f))\n",
    "                df = df.merge(a, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  process_feature(train_x, valid_x, test_df):\n",
    "    result = []\n",
    "    drop_cols = ['id','label']\n",
    "    for df in [train_x, valid_x, test_df]:\n",
    "        result.append(df.drop(drop_cols, axis=1))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def cv(df, num_folds, param, model_dir, stratified=True, debug=False):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    train_df = df[df.label.notnull()]\n",
    "    test_df = df[df.label.isnull()]\n",
    "#     seed = param[\"seed\"]\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=178)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=178)\n",
    "#     del param['seed']\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    all_test_preds = []    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_df['label'])):\n",
    "        train_x, train_y = train_df.iloc[train_idx], train_df['label'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df.iloc[valid_idx], train_df['label'].iloc[valid_idx]\n",
    "        fold_preds = test_df[[\"id\"]]\n",
    "        \n",
    "        train_x, valid_x, test = process_feature(train_x, valid_x, test_df)\n",
    "        if n_fold == 0:\n",
    "            print(train_x.shape, valid_x.shape, test.shape)\n",
    "        \n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        validation_data = lgb.Dataset(valid_x, label=valid_y)\n",
    "\n",
    "        clf=lgb.train(params,\n",
    "                      train_data,\n",
    "                      num_boost_round=10000,\n",
    "                      valid_sets=[train_data, validation_data],\n",
    "                      valid_names=[\"train\", \"valid\"],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=100)\n",
    "\n",
    "        valid_preds = clf.predict(valid_x, num_iteration=clf.best_iteration)\n",
    "        test_preds = clf.predict(test, num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_preds['prob'] = test_preds\n",
    "        fold_preds['fold_id'] = n_fold + 1\n",
    "        all_test_preds.append(fold_preds)\n",
    "        \n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        \n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, valid_preds)))\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    full_auc = roc_auc_score(train_df['label'], oof_preds)\n",
    "    print('Full AUC score %.6f' % full_auc)    \n",
    "    \n",
    "    if not debug:\n",
    "        train_df[\"prob\"] = oof_preds\n",
    "        train_df[['id', 'prob']].to_csv(model_dir + \"pred_train.csv\", index= False)\n",
    "\n",
    "        all_test_preds = pd.concat(all_test_preds, axis=0)\n",
    "        all_test_preds.to_csv(model_dir + \"all_test_preds.csv\", index=False)\n",
    "        \n",
    "        sub = pd.DataFrame()\n",
    "        sub['id'] = all_test_preds.id.unique()\n",
    "        sub.set_index(\"id\", inplace=True)\n",
    "        sub[\"prob\"] = all_test_preds.groupby(\"id\").prob.mean()\n",
    "        sub.reset_index().to_csv(model_dir + \"sub.txt\", index=False)\n",
    "    return full_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"../model_output/lgb_572\"):\n",
    "    for root, dirs, files in os.walk(\"../model_output/lgb_572\", topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_list = [\n",
    "     {'boosting_type': 'goss', 'colsample_bytree': 0.7781932815594309, 'learning_rate': 0.01624013644582742, 'max_bin': 840, 'metric': 'auc', 'min_child_weight': 5.318216087038529, 'num_leaves': 147, 'reg_alpha': 18.08297296305982, 'reg_lambda': 789.1007316608059, 'subsample': 1.0},\n",
    "     {'boosting_type': 'goss', 'colsample_bytree': 0.837178619459219, 'learning_rate': 0.015280005792126359, 'max_bin': 940, 'metric': 'auc', 'min_child_weight': 3.642932064685643, 'num_leaves': 200, 'reg_alpha': 12.582785727080129, 'reg_lambda': 814.1008563896332, 'subsample': 1.0},\n",
    "     {'boosting_type': 'goss', 'colsample_bytree': 0.8358720625479714, 'learning_rate': 0.015534782170846308, 'max_bin': 840, 'metric': 'auc', 'min_child_weight': 3.1659707148760523, 'num_leaves': 184, 'reg_alpha': 8.304429696146045, 'reg_lambda': 799.0109645277794, 'subsample': 1.0},\n",
    "     {'boosting_type': 'goss', 'colsample_bytree': 0.7847476626113957, 'learning_rate': 0.018212798578344867, 'max_bin': 880, 'metric': 'auc', 'min_child_weight': 6.812288944466467, 'num_leaves': 153, 'reg_alpha': 27.63694670558659, 'reg_lambda': 937.4604258901295, 'subsample': 1.0},\n",
    "     {'boosting_type': 'goss', 'colsample_bytree': 0.8339891785243042, 'learning_rate': 0.015453208130924444, 'max_bin': 500, 'metric': 'auc', 'min_child_weight': 4.6719481396446945, 'num_leaves': 171, 'reg_alpha': 10.029404409993981, 'reg_lambda': 897.7049086437637, 'subsample': 1.0}\n",
    "]\n",
    "result = []\n",
    "for i, params in enumerate(params_list):\n",
    "    model_dir = \"../model_output/lgb_572/%d/\" % i\n",
    "    result.append(cv(df, 5, params, model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
